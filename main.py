import streamlit as st
from config.config import APP_TITLE, APP_ICON, LAYOUT, INITIAL_SIDEBAR_STATE

# å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title=APP_TITLE,
    page_icon=APP_ICON,
    layout=LAYOUT,
    initial_sidebar_state=INITIAL_SIDEBAR_STATE
)

# åœ¨é¡µé¢é¡¶éƒ¨æ·»åŠ æ ‡é¢˜
st.title(APP_TITLE)

# è®¾ç½® Pydantic é…ç½®
from typing import Any, Dict
import pydantic
from pydantic import BaseModel

# è®¾ç½®å…¨å±€ Pydantic é…ç½®
pydantic.config.ConfigDict.arbitrary_types_allowed = True

# æ·»åŠ åŸºç¡€é…ç½®ç±»
class BaseConfig(BaseModel):
    class Config:
        arbitrary_types_allowed = True
        json_encoders = {
            # æ·»åŠ è‡ªå®šä¹‰çš„ JSON ç¼–ç å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        }

# ç¡®ä¿æ‰€æœ‰ä½¿ç”¨ Pydantic çš„ç±»éƒ½ç»§æ‰¿è¿™ä¸ªé…ç½®
class Config(BaseConfig):
    pass

# æ ‡å‡†åº“
import os
import json
import logging
import shutil
from pathlib import Path
from contextlib import contextmanager


# äº‹ä»¶å¾ªç¯å¤„ç†
import asyncio
from functools import partial
from concurrent.futures import ThreadPoolExecutor

# åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨
thread_pool = ThreadPoolExecutor(max_workers=4)

# ç¡®ä¿åªæœ‰ä¸€ä¸ªäº‹ä»¶å¾ªç¯
try:
    loop = asyncio.get_running_loop()
except RuntimeError:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# è®¾ç½®çº¿ç¨‹æ± æ‰§è¡Œå™¨
loop.set_default_executor(thread_pool)  # ä½¿ç”¨æ˜¾å¼çš„çº¿ç¨‹æ± æ‰§è¡Œå™¨

# é˜²æ­¢ Streamlit åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
def run_async(coro):
    """è¿è¡Œå¼‚æ­¥ä»£ç çš„åŒ…è£…å™¨"""
    try:
        return loop.run_until_complete(coro)
    except RuntimeError as e:
        if "Event loop is closed" in str(e):
            # å¦‚æœå¾ªç¯å·²å…³é—­ï¼Œåˆ›å»ºæ–°çš„å¾ªç¯
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            return new_loop.run_until_complete(coro)
        raise


# ========================== #
#       ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
# ========================== #
# ç¬¬ä¸‰æ–¹åº“
import langchain
langchain.verbose = False

# é€‚é…æ–°ç‰ˆæœ¬ langchain ç»“æ„
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitters import RecursiveCharacterTextSplitter

try:
    from langchain.chains import ConversationalRetrievalChain
except ImportError:
    from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain


from langchain_community.vectorstores import Chroma as ChromaDB

try:
    from langchain_community.embeddings.ollama import OllamaEmbeddings
except ImportError:
    from langchain_community.embeddings import OllamaEmbeddings

# æ–‡æ¡£åŠ è½½å™¨
try:
    from langchain_community.document_loaders.pdf import PyPDFLoader
except ImportError:
    from langchain_community.document_loaders import PyPDFLoader

from langchain.schema import (
    Document,
    HumanMessage,
    AIMessage
)

# é…ç½®æ¨¡å—
try:
    from config.config import (
        VECTOR_DB_PATH,
        APP_TITLE,
        APP_ICON,
        AVAILABLE_MODELS,
        DEFAULT_MODEL,
        KNOWLEDGE_BASE_PATH
    )
except ModuleNotFoundError:
    raise ImportError("âš ï¸ è¯·ç¡®ä¿ config/config.py å­˜åœ¨")

## ğŸŒŸ ç»„ä»¶ç›¸å…³
from components.ui import (
    setup_sidebar,
    file_selector,
    show_file_upload,
    show_system_status,
    display_chat_message 
)
from components.chat import (
    initialize_chat_history,
    get_chat_qa_chain,
    generate_response,
    display_source_documents
)
from components.file_manager import (
    create_file_manager,
    initialize_file_manager,
    show_file_manager_dialog
)

## ğŸŒŸ å·¥å…·ç±»
from utils.api_handler import APIHandler
from utils.model_utils import get_llm, get_embedding_model
from utils.file_utils import load_documents, get_knowledge_base_files
from utils.manager_utils import ensure_knowledge_base_structure
from utils.vectordb_utils import (
    add_documents_to_vectordb,
    rebuild_vectordb_for_files,
    get_vectordb
)
from utils.logger import logger
# æ•°æ®åº“è¿æ¥
try:
    from services.database import SessionLocal, Base, engine
except ModuleNotFoundError:
    raise ImportError("âš ï¸ `services/database.py` å¯èƒ½ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
from services.google_drive_service import GoogleDriveService


# åˆ›å»ºæ•°æ®åº“è¡¨
Base.metadata.create_all(bind=engine)

def get_db():
    db = SessionLocal()
    try:
        return db
    finally:
        db.close()

def init_drive_service():
    """åˆå§‹åŒ– Google Drive æœåŠ¡"""
    if 'drive_service' not in st.session_state:
        st.session_state.drive_service = GoogleDriveService()

def show_vector_store_management():
    st.header("å‘é‡åº“ç®¡ç†")
    
    # å¢é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“
    st.button("å¢é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“")
    
    # å‘é‡åº“ç»´æŠ¤è¯´æ˜
    with st.expander("å‘é‡åº“ç»´æŠ¤è¯´æ˜"):
        st.write("""
        1. å‘é‡åº“ç”¨äºå­˜å‚¨æ–‡æ¡£çš„è¯­ä¹‰è¡¨ç¤º
        2. æ”¯æŒå¢é‡æ·»åŠ æ–°æ–‡æ¡£
        3. å®šæœŸç»´æŠ¤å¯ç¡®ä¿æœ€ä½³æ€§èƒ½
        """)

def show_feature_selection():
    st.header("åŠŸèƒ½é€‰æ‹©")
    
    # Google DriveåŒæ­¥é€‰é¡¹
    drive_sync = st.checkbox("Google Drive åŒæ­¥")
    
    if drive_sync:
        show_drive_sync()

def show_drive_sync():
    st.subheader("Google Drive åŒæ­¥")
    
    init_drive_service()
    
    if st.button("åŒæ­¥ Google Drive"):
        try:
            # ç¡®ä¿å·²è®¤è¯
            st.session_state.drive_service.authenticate()
            
            # æ‰§è¡ŒåŒæ­¥
            success = st.session_state.drive_service.sync_drive_files()
            
            if success:
                st.success("åŒæ­¥å®Œæˆï¼")
                # è‡ªåŠ¨æ›´æ–°å‘é‡åº“
                with st.spinner("æ­£åœ¨æ›´æ–°å‘é‡åº“..."):
                    
                    try:
                        # ä½¿ç”¨å·²ç»éªŒè¯å¯ç”¨çš„å‡½æ•°è·å–æ–‡ä»¶
                        files = get_knowledge_base_files()
                        if files:
                            vectordb = add_documents_to_vectordb(files)
                            if vectordb:
                                st.success("å‘é‡åº“æ›´æ–°æˆåŠŸï¼")
                            else:
                                st.error("å‘é‡åº“æ›´æ–°å¤±è´¥")
                        else:
                            st.warning("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶")
                    except Exception as e:
                        st.error(f"æ›´æ–°å‘é‡åº“æ—¶å‡ºé”™: {str(e)}")
        except Exception as e:
            st.error(f"åŒæ­¥å¤±è´¥: {str(e)}")

def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = DEFAULT_MODEL
    if "api_key" not in st.session_state:
        st.session_state.api_key = ""
    if "user_question" not in st.session_state:
        st.session_state.user_question = ""
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "show_file_manager" not in st.session_state:
        st.session_state.show_file_manager = False
    if "selected_docs_vectordb" not in st.session_state:
        st.session_state.selected_docs_vectordb = None

def clean_response(response):
    """æ¸…ç†å“åº”æ–‡æœ¬ï¼Œå»é™¤ä¸éœ€è¦çš„ç¬¦å·å’Œå…ƒæ•°æ®"""
    if isinstance(response, dict):
        content = response.get("answer") or response.get("content") or str(response)
    else:
        content = str(response)
    
    # æ¸…ç†å“åº”æ–‡æœ¬
    if "additional_kwargs=" in content:
        content = content.split("additional_kwargs=")[0]  # ç§»é™¤å…ƒæ•°æ®éƒ¨åˆ†
    
    # ç§»é™¤ think æ ‡ç­¾
    content = content.replace("content='<think>", "")
    # content = content.replace("</think>", "")
    content = content.replace("<think>", "")
    content=content.replace("content='", "")
    content = content.replace("\\n", "\n")  # å¤„ç†æ¢è¡Œç¬¦
    content = content.strip("'\" ")  # ç§»é™¤é¦–å°¾çš„å¼•å·å’Œç©ºæ ¼
    return content

def clear_text():
    """æ¸…ç©ºè¾“å…¥æ¡†çš„å›è°ƒå‡½æ•°"""
    if st.session_state.user_input:
        st.session_state.user_question = st.session_state.user_input
        st.session_state.user_input = ""  # åªæ¸…ç©ºè¾“å…¥æ¡†ï¼Œä¸æ¸…ç©ºé—®é¢˜

def generate_response(llm, prompt, search_results=None, source_type=None):
    """ç”Ÿæˆå›ç­”"""
    try:
        response = llm.invoke(prompt)
        if response is None:
            return "âš ï¸ AI æ²¡æœ‰è¿”å›ç»“æœï¼Œè¯·ç¨åå†è¯•", [], source_type
        
        # å¦‚æœæä¾›äº†æœç´¢ç»“æœï¼Œå°†å…¶è½¬æ¢ä¸ºåŸæ–‡æ¡£æ ¼å¼
        source_documents = []
        
        if search_results and 'google' in search_results and 'data' in search_results['google']:
            for item in search_results['google']['data'][:5]:
                title = item.get("title", "")
                snippet = item.get("snippet", "")
                link = item.get("link", "")
                
                # åˆ›å»ºæºæ–‡æ¡£å¯¹è±¡
                source_documents.append(
                    Document(
                        page_content=f"ğŸ“Œ {title}\nğŸ“„ æ‘˜è¦ï¼š{snippet}\nğŸ”— æ¥æºï¼š{link}\n",
                        metadata={"source": link, "title": title}
                    )
                )
        
        return clean_response(response), source_documents, source_type
    except Exception as e:
        logger.error(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return f"âš ï¸ å‘ç”Ÿé”™è¯¯ï¼Œæ— æ³•ç”Ÿæˆå›ç­”: {str(e)}", [], source_type

def format_session_state():
    """æ ¼å¼åŒ– session_state ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²æ ¼å¼"""
    try:
        
        formatted_lines = []
        for key, value in st.session_state.items():
            try:
                if isinstance(value, (str, int, float, bool, type(None))):
                    formatted_value = str(value)
                elif hasattr(value, '__class__'):
                    class_name = value.__class__.__name__
                    if class_name == 'UploadedFile':
                        formatted_value = f"UploadedFile(name={value.name})"
                    else:
                        formatted_value = f"{class_name}({str(value)})"
                else:
                    formatted_value = str(value)
                
                formatted_lines.append(f"{key}: {formatted_value}")
            except Exception as e:
                formatted_lines.append(f"{key}: <é”™è¯¯: {str(e)}>")
        
        return "\n".join(formatted_lines)
    except Exception as e:
        return f"æ ¼å¼åŒ– session state æ—¶å‡ºé”™: {str(e)}"

# åˆ›å»ºä¸€ä¸ªç”¨æˆ·å‹å¥½çš„çŠ¶æ€æ˜¾ç¤º
def show_user_friendly_status(message=None):
    # åˆ›å»ºä¸€ä¸ªçŠ¶æ€å®¹å™¨
    if "status_container" not in st.session_state:
        st.session_state.status_container = st.empty()
    
    # å¦‚æœæœ‰æ¶ˆæ¯ï¼Œæ˜¾ç¤ºä¸€ä¸ªå‹å¥½çš„åŠ è½½æŒ‡ç¤ºå™¨
    if message:
        with st.session_state.status_container:
            st.markdown(f"<div class='stProgress'><div class='stProgressIndicator'>â³</div> {message}</div>", 
                       unsafe_allow_html=True)
    else:
        # æ¸…ç©ºçŠ¶æ€å®¹å™¨
        st.session_state.status_container.empty()
        
# show_user_friendly_status('æ­£åœ¨åŠ è½½...')
# show_user_friendly_status('åŠ è½½å®Œæˆ')


def handle_free_chat(llm, user_question, chat_history):
    """å¤„ç†è‡ªç”±å¯¹è¯çš„ä¸»è¦é€»è¾‘"""
    logger.info("å¼€å§‹è‡ªç”±å¯¹è¯æ¨¡å¼å¤„ç†")

    # è·å–æ ¼å¼åŒ–çš„å¯¹è¯å†å²
    formatted_history = format_chat_history(chat_history)

    # **ç¬¬ä¸€æ­¥ï¼šLLM ç›´æ¥å›ç­”**
    answer, source_documents = get_llm_direct_response(llm, user_question, formatted_history)

    # **ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥ LLM æ˜¯å¦ç¡®å®šç­”æ¡ˆ**
    if should_perform_search(user_question, answer):
        try:
            logger.info(f"ğŸ” è§¦å‘æœç´¢ï¼ŒæŸ¥è¯¢ï¼š{user_question}")

            # **æ‰§è¡Œæœç´¢**
            search_results = perform_web_search(user_question)

            # **æ£€æŸ¥æœç´¢æ˜¯å¦æˆåŠŸ**
            if search_results:
                logger.info(f"âœ… æˆåŠŸè·å–æœç´¢ç»“æœ: {search_results}")

                # **å¦‚æœ LLM å…ˆå›ç­”äº†å†…å®¹ï¼ŒæŠŠ LLM çš„å›ç­”ä¹Ÿæ”¾è¿› Prompt**
                search_prompt = f"""
                {formatted_history}
                
                ã€å½“å‰é—®é¢˜ã€‘
                {user_question}

                ã€AIåˆæ­¥å›ç­”ã€‘ï¼ˆå¦‚æœå¯ä¿¡ï¼Œå¯ä»¥å‚è€ƒï¼‰
                {answer}

                ğŸ”ã€æœ€æ–°æœç´¢ç»“æœã€‘ï¼š
                {format_search_results(search_results)}

                ğŸ“‹ ã€å›ç­”è¦æ±‚ã€‘ï¼š
                1. **ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºæœç´¢ç»“æœ**
                2. **ä½ ä¸èƒ½ä½¿ç”¨ LLM å†…éƒ¨çŸ¥è¯†ï¼Œé™¤éæœç´¢ç»“æœä¸è¶³**
                3. **è¯·ç›´æ¥å¼•ç”¨æœç´¢ç»“æœä¸­çš„æœ€æ–°ä¿¡æ¯ï¼Œå¹¶æä¾›æ¥æº**
                4. **å¦‚æœæœç´¢ç»“æœæåˆ°å…·ä½“æ—¶é—´å’Œäººç‰©ï¼Œä½ å¿…é¡»ä½¿ç”¨è¿™äº›ä¿¡æ¯**
                """

                # **è®© LLM ç»“åˆæœç´¢ä¿¡æ¯é‡æ–°å›ç­”**
                new_answer, new_source_docs = generate_response(llm, search_prompt, search_results, "web_search")[:2]
                
                if new_answer:
                    answer = new_answer
                    source_documents = new_source_docs

        except Exception as e:
            logger.error(f"âŒ æœç´¢å¢å¼ºå›ç­”å¤±è´¥: {str(e)}", exc_info=True)
            # **æœç´¢å¤±è´¥æ—¶ï¼Œä»ç„¶ä½¿ç”¨ LLM å›ç­”**
    
    return answer, source_documents

def format_chat_history(chat_history):
    """æ ¼å¼åŒ–å¯¹è¯å†å²"""
    if not chat_history:
        return ""
    
    history_messages = []
    for msg in chat_history[-5:]:  # åªä¿ç•™æœ€è¿‘5æ¡å¯¹è¯
        prefix = "ç”¨æˆ·ï¼š" if msg['role'] == "user" else "AIï¼š"
        history_messages.append(f"{prefix}{msg['content']}")
    
    return "ã€å†å²å¯¹è¯ã€‘\n" + "\n".join(history_messages) + "\n"

def get_llm_direct_response(llm, user_question, formatted_history):
    """è·å–LLMçš„ç›´æ¥å›ç­”"""
    logger.info("å°è¯• LLM ç›´æ¥å›ç­”")
    
    prompt = f"""
    {formatted_history}
    
    ã€å½“å‰é—®é¢˜ã€‘
    {user_question}

    ğŸ“‹ ã€å›ç­”è¦æ±‚ã€‘ï¼š
    1. **å¦‚æœä½ èƒ½100%ç¡®å®šç­”æ¡ˆï¼Œè¯·ç›´æ¥å›ç­”**
    2. **å¦‚æœé—®é¢˜æ¶‰åŠæœ€æ–°ä¿¡æ¯ï¼ˆå¦‚â€œç°åœ¨â€ã€â€œä»Šå¹´â€ã€â€œæœ€è¿‘â€ï¼‰ï¼Œè¯·ç›´æ¥è¯´ï¼š"è¯·æŸ¥è¯¢æœ€æ–°æ•°æ®"**
    3. **å¦‚æœä½ ä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ï¼š"æˆ‘ä¸ç¡®å®šï¼Œè¯·æŸ¥è¯¢æœ€æ–°ä¿¡æ¯"**
    4. **ä¸è¦ç¼–é€ ä¿¡æ¯**
    """
    
    answer, source_documents = generate_response(llm, prompt, None, "direct")[:2]

    # âœ… è®°å½• LLM çš„å›ç­”ï¼Œä»¥ä¾¿è°ƒè¯•
    logger.info(f"ğŸ’¬ LLM ç›´æ¥å›ç­”: {answer}")

    return answer, source_documents

def should_perform_search(user_question, llm_answer):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›è¡Œæœç´¢"""
    # æ—¶æ•ˆæ€§å…³é”®è¯
    time_keywords = [
        "æœ€æ–°", "ç°åœ¨", "ä»Šå¤©", "æœ€è¿‘", "ç›®å‰",
        "2025", "æ˜å¹´", "ä»Šå¹´", "ä¸Šä¸ªæœˆ", "è¿™ä¸ªæœˆ"
    ]
    
    # éœ€è¦å®æ—¶æ•°æ®çš„ä¸»é¢˜
    topic_keywords = [
        "è‚¡ç¥¨", "åŸºé‡‘", "æˆ¿ä»·", "æ²¹ä»·", "æ±‡ç‡",
        "å¤©æ°”", "ç–«æƒ…", "æ–°é—»", "æ”¿ç­–", "è¡Œæƒ…",
        "ä»·æ ¼", "ä¸Šå¸‚", "å‘å¸ƒ", "æ›´æ–°", "å…¬å‘Š"
    ]
    
    # LLM è¡¨ç¤ºä¸ç¡®å®šçš„æ ‡å¿—
    uncertainty_signals = [
        "æˆ‘ä¸ç¡®å®š", "éœ€è¦æŸ¥è¯¢", "éœ€è¦æ ¸å®",
        "æœ€æ–°ä¿¡æ¯", "å®æ—¶æ•°æ®", "å»ºè®®æŸ¥è¯¢"
    ]
    
    needs_search = (
        any(keyword in user_question for keyword in time_keywords) or
        any(keyword in user_question for keyword in topic_keywords) or
        any(signal in llm_answer for signal in uncertainty_signals)
    )
    
    if needs_search:
        logger.info(f"ğŸ” è§¦å‘æœç´¢åŸå› : é—®é¢˜æˆ–å›ç­”ä¸­åŒ…å«éœ€è¦å®æ—¶ä¿¡æ¯çš„å…³é”®è¯")
    
    return needs_search

def perform_web_search(query):
    """æ‰§è¡Œç½‘ç»œæœç´¢"""
    try:
        api_handler = APIHandler()  # ç¡®ä¿ APIHandler å·²æ­£ç¡®é…ç½®
        search_results = api_handler.search_web(query)
        
        if not search_results or 'google' not in search_results or 'data' not in search_results['google']:
            logger.warning("æœç´¢è¿”å›æ•°æ®æ ¼å¼é”™è¯¯æˆ–æ— ç»“æœ")
            return None

        filtered_results = search_results['google']['data'][:5]
        if not filtered_results:
            logger.warning("æœç´¢ç»“æœä¸ºç©º")

        # âœ… æ‰“å°æ—¥å¿—ï¼Œæ£€æŸ¥æœç´¢ç»“æœæ˜¯å¦æœ‰æ•ˆ
        logger.info(f"ğŸ” æœç´¢ç»“æœï¼š{filtered_results}")
        
        return filtered_results
            
    except Exception as e:
        logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}", exc_info=True)
        return None

def format_search_results(search_results):
    """æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œç¡®ä¿ LLM èƒ½æ­£ç¡®è§£æ"""
    if not search_results:
        return "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
        
    formatted_results = []
    
    for item in search_results:
        title = item.get("title", "æœªçŸ¥æ ‡é¢˜")
        snippet = item.get("snippet", "æš‚æ— æ‘˜è¦")
        link = item.get("link", "#")
        
        formatted_results.append(f"""
ğŸ“Œ **{title}**
ğŸ“ **æ‘˜è¦**ï¼š{snippet}
ğŸ”— **æ¥æº**ï¼š[{link}]({link})
""")
    
    return "\n".join(formatted_results)

def main():
    initialize_session_state()
    
    # è°ƒè¯•ä¿¡æ¯
    # logger.debug(f"Session State:\n{format_session_state()}")
    
    # æ£€æŸ¥å…³é”®å˜é‡
    if "selected_model" not in st.session_state:
        st.error("æœªåˆå§‹åŒ– `selected_model`ï¼Œæ­£åœ¨ä½¿ç”¨é»˜è®¤å€¼")
        st.session_state.selected_model = DEFAULT_MODEL
    if "api_key" not in st.session_state:
        st.session_state.api_key = ""
        
    # è®¾ç½®ä¾§è¾¹æ 
    selected_model, api_key = setup_sidebar()
    
   # æ£€æŸ¥Google DriveåŒæ­¥æ˜¯å¦è¢«é€‰ä¸­
    if hasattr(st.session_state, 'drive_sync_checked') and st.session_state.drive_sync_checked:
        # ä½¿ç”¨å ä½ç¬¦æ˜¾ç¤ºGoogle DriveåŒæ­¥å†…å®¹
        with st.session_state.drive_sync_content.container():
            show_drive_sync()
            
            st.markdown("---")  # æ·»åŠ åˆ†éš”çº¿
        
    
    # èŠå¤©æ¨¡å¼é€‰æ‹©
    chat_mode = st.radio(
        "é€‰æ‹©èŠå¤©æ¨¡å¼",
        ["çŸ¥è¯†åº“å¯¹è¯", "æ–‡æ¡£å¯¹è¯", "è‡ªç”±å¯¹è¯"],
        key="chat_mode"
    )
    
    # æ–‡æ¡£å¯¹è¯æ¨¡å¼çš„æ–‡ä»¶é€‰æ‹©
    if chat_mode == "æ–‡æ¡£å¯¹è¯":
        st.info("è¯·é€‰æ‹©è¦å¯¹è¯çš„æ–‡æ¡£")
        # è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
        available_files = get_knowledge_base_files()
        
        if not available_files:
            st.warning("æš‚æ— å¯ç”¨æ–‡æ¡£ï¼Œè¯·å…ˆåœ¨ä¾§è¾¹æ ä¸Šä¼ æ–‡æ¡£")
        else:
            selected_files = st.multiselect(
                "é€‰æ‹©è¦å¯¹è¯çš„æ–‡æ¡£ï¼ˆå¯å¤šé€‰ï¼‰",
                options=available_files,
                format_func=lambda x: os.path.basename(x)
            )
            
            if selected_files:
                with st.spinner("å‡†å¤‡æ–‡æ¡£å¯¹è¯..."):
                    try:
                        # ä½¿ç”¨æ°¸ä¹…å‘é‡åº“è€Œä¸æ˜¯åˆ›å»ºä¸´æ—¶å‘é‡åº“
                        if not VECTOR_DB_PATH.exists():
                            st.error("å‘é‡åº“è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ„å»ºå‘é‡åº“")
                            st.stop()
                        
                        # åŠ è½½æ°¸ä¹…å‘é‡åº“
                        vectordb = ChromaDB(
                            persist_directory=str(VECTOR_DB_PATH),
                            embedding_function=get_embedding_model()
                        )
                        
                        # åˆ›å»ºè¿‡æ»¤å™¨ï¼Œåªæ£€ç´¢é€‰å®šçš„æ–‡æ¡£
                        file_filter = {"source": {"$in": selected_files}}
                        
                        # ä½¿ç”¨è¿‡æ»¤å™¨åˆ›å»ºæ£€ç´¢å™¨
                        retriever = vectordb.as_retriever(
                            search_kwargs={
                                "k": 5,
                                "filter": file_filter  # åº”ç”¨è¿‡æ»¤å™¨
                            }
                        )
                        
                        # å­˜å‚¨æ£€ç´¢å™¨è€Œä¸æ˜¯æ•´ä¸ªå‘é‡åº“
                        st.session_state.selected_docs_retriever = retriever
                        
                        # st.success(f"æˆåŠŸå‡†å¤‡ {len(selected_files)} ä¸ªæ–‡æ¡£ï¼")
                            
                    except Exception as e:
                        st.error(f"å‡†å¤‡æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
                        st.session_state.selected_docs_retriever = None
    
    # å¯¹è¯åŒºåŸŸ
    st.markdown("### å¯¹è¯åŒºåŸŸ")
    
    if chat_mode == "è‡ªç”±å¯¹è¯":
        st.info("åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥ä¸AIåŠ©æ‰‹è¿›è¡Œè‡ªç”±å¯¹è¯ï¼Œä¸å—çŸ¥è¯†åº“é™åˆ¶ã€‚")
    
    # åˆ›å»ºå¯¹è¯å®¹å™¨å’Œè¾“å…¥åŒºåŸŸ
    chat_container = st.container()
    with st.container():
        user_input = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", key="user_input", on_change=clear_text)
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    with chat_container:
        for message in st.session_state.get("chat_history", []):
            if message["role"] == "user":
                display_chat_message("user", message["content"])
            else:
                display_chat_message("assistant", message["content"])
                if "source_documents" in message:
                    display_source_documents(message["source_documents"])
            st.markdown("---")
    
    # å¤„ç†ç”¨æˆ·é—®é¢˜
    if st.session_state.user_question:
        try:
            with st.spinner():
                llm = get_llm(st.session_state.selected_model, st.session_state.api_key)
                if not llm:
                    st.error("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•åŠ è½½ LLM")
                    st.stop()
                
                if chat_mode == "è‡ªç”±å¯¹è¯":
                    try:
                        answer, source_documents = handle_free_chat(
                            llm,
                            st.session_state.user_question,
                            st.session_state.get("chat_history", [])
                        )
                    except Exception as e:
                        logger.error(f"è‡ªç”±å¯¹è¯å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
                        answer = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•æ­£ç¡®å¤„ç†æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨åå†è¯•"
                        source_documents = []
                        
                elif chat_mode == "çŸ¥è¯†åº“å¯¹è¯":
                    try:
                        if not VECTOR_DB_PATH.exists():
                            st.error("çŸ¥è¯†åº“è·¯å¾„ä¸å­˜åœ¨")
                            st.stop()
                            
                        # å°è¯•ä½¿ç”¨å†…å­˜æ¨¡å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                        try:
                            # å…ˆå°è¯•åŠ è½½æŒä¹…åŒ–å‘é‡åº“
                            vectordb = ChromaDB(
                                persist_directory=str(VECTOR_DB_PATH),
                                embedding_function=get_embedding_model()
                            )
                        except Exception as e:
                            st.warning(f"åŠ è½½æŒä¹…åŒ–å‘é‡åº“å¤±è´¥: {str(e)}")
                            logger.info("æ­£åœ¨ä½¿ç”¨å†…å­˜æ¨¡å¼ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ...")
                            
                            # è·å–æ‰€æœ‰æ–‡æ¡£
                            files = get_knowledge_base_files()
                            if not files:
                                st.warning("çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡ä»¶")
                                st.stop()
                                
                            # åŠ è½½æ–‡æ¡£
                            documents = load_documents(files)
                            
                            # ä½¿ç”¨å†…å­˜æ¨¡å¼
                            client = chromadb.Client()
                            vectordb = ChromaDB(
                                client=client,
                                collection_name="knowledge_base",
                                embedding_function=get_embedding_model()
                            )
                            
                            # æ·»åŠ æ–‡æ¡£
                            vectordb.add_documents(documents)
                            
                        # åˆ›å»ºé—®ç­”é“¾
                        qa_chain = get_chat_qa_chain(llm, vectordb)
                        response = qa_chain.invoke({"question": st.session_state.user_question})
                        answer = response['answer']
                        source_documents = response.get('source_documents', [])
                    except Exception as e:
                        logger.error(f"å¤„ç†çŸ¥è¯†åº“å¯¹è¯æ—¶å‡ºé”™: {str(e)}", exc_info=True)
                        st.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
                        st.stop()
                else:  # æ–‡æ¡£å¯¹è¯æ¨¡å¼
                    if not st.session_state.selected_docs_retriever:
                        st.warning("è¯·å…ˆé€‰æ‹©æ–‡æ¡£")
                        st.stop()
                    qa_chain = ConversationalRetrievalChain.from_llm(
                        llm=llm,
                        retriever=st.session_state.selected_docs_retriever,
                        memory=ConversationBufferMemory(
                            memory_key="chat_history",
                            return_messages=True
                        )
                    )
                    response = qa_chain({"question": st.session_state.user_question})
                    answer = response['answer']
                    source_documents = response.get('source_documents', [])

                # æ›´æ–°å¯¹è¯å†å²
                st.session_state.chat_history.extend([
                    {"role": "user", "content": st.session_state.user_question},
                    {"role": "assistant", "content": answer, "source_documents": source_documents}
                ])
                logger.info("å¯¹è¯å†å²å·²æ›´æ–°")
                
                # æ¸…ç©ºç”¨æˆ·é—®é¢˜
                st.session_state.user_question = ""
                
                # åˆ·æ–°é¡µé¢
                st.rerun()

        except Exception as e:
            error_msg = f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}"
            logger.error(error_msg, exc_info=True)
            st.error(error_msg)
            st.session_state.user_question = ""


if __name__ == "__main__":
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("logs/app.log"),
            # å¦‚æœä½ ä¸æƒ³åœ¨æ§åˆ¶å°çœ‹åˆ°æ—¥å¿—ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
            logging.StreamHandler()
        ]
    )

    # è·å–logger
    logger = logging.getLogger("knowledge_base")

    main() 