try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

import streamlit as st
import os
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from zhipuai_embedding import ZhipuAIEmbeddings
# from langchain.vectorstores.chroma import Chroma
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv
from langchain_community.document_loaders.pdf import PyMuPDFLoader
from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# import io
# import fitz  # PyMuPDF
from langchain.schema import Document
from PyPDF2 import PdfReader
from zhipuai_llm import ZhipuAILLM

load_dotenv()  # è¯»å– .env æ–‡ä»¶
zhipuai_api_key = os.getenv('ZHIPUAI_API_KEY')

if not zhipuai_api_key:
    st.error("æœªæ‰¾åˆ° ZHIPUAI_API_KEYã€‚è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®äº†æ­£ç¡®çš„ API keyã€‚")
    st.stop()

def generate_response(input_text, zhipuai_api_key):
    llm = ZhipuAILLM(
                    model= "glm-4-flash",
                     temperature=0.7, 
                     api_key=zhipuai_api_key)
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    #st.info(output)
    return output

# def get_vectordb_disk():
#     # å®šä¹‰ Embeddings
#     embedding = ZhipuAIEmbeddings()
#     # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
#     persist_directory = 'data_base/vector_db/chroma'
#     # åŠ è½½æ•°æ®åº“
#     vectordb = Chroma(
#         persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
#         embedding_function=embedding
#     )
        
#     return vectordb

# def get_vectordb_memory(split_docs):
#     # å®šä¹‰ Embeddings
#     embedding = ZhipuAIEmbeddings()
#     # åŠ è½½æ•°æ®åº“
#     vectordb = Chroma.from_documents(
#         documents=split_docs,
#         embedding=embedding
#     )
#     return vectordb

# # è·å–vectordb
# def get_vectordb(uploaded_files):
#     embedding = ZhipuAIEmbeddings()
#     persist_directory = 'data_base/vector_db/chroma'
#     if uploaded_files:
#         documents = []
#         for uploaded_file in uploaded_files:
#             if uploaded_file is not None:
#                 # PDFä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°,å¹¶é€šè¿‡æ–‡ä»¶è·¯å¾„åŠ è½½æ–‡æ¡£
#                 # path = os.path.join("data_base/knowledge_db", uploaded_file.name)
#                 # with open(path, "wb") as f:
#                 #     f.write(uploaded_file.getbuffer())
#                 # documents = load_pdf(path)


#                 # # ä»ç¼“å­˜ä¸­è¯»å– PDF æ–‡ä»¶å†…å®¹ æ–¹æ³•ä¸€
#                 # pdf_bytes = uploaded_file.read()
#                 # pdf_stream = io.BytesIO(pdf_bytes)
#                 # pdf_document = fitz.open(stream=pdf_stream, filetype="pdf")

#                     # # æå–æ–‡æœ¬å†…å®¹
#                 # text = ""
#                 # for page_num in range(len(pdf_document)):
#                 #     page = pdf_document.load_page(page_num)
#                 #     text += page.get_text()

#                 # ä»ç¼“å­˜ä¸­è¯»å– PDF æ–‡ä»¶å†…å®¹ æ–¹æ³•äºŒ
#                 pdf_document = PdfReader(uploaded_file)
#                     # æå–æ–‡æœ¬å†…å®¹
#                 text = ""
#                 for page in pdf_document.pages:
#                     text += page.extract_text()

#                 # åˆ›å»º langchain æ–‡æ¡£å¯¹è±¡
#                 document = Document(page_content=text)
#                 documents.append(document)
#                 st.sidebar.success(f"{uploaded_file.name} has been successfully uploaded.")
#             else:
#                 st.sidebar.error(f"{uploaded_file.name} is not a valid file.")

#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=500, chunk_overlap=50)

#         split_docs = text_splitter.split_documents(documents)

#         vectordb = Chroma.from_documents(
#                                             documents=split_docs,
#                                             embedding=embedding)
#     else:
#         vectordb = Chroma(
#                             persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
#                             embedding_function=embedding)
#     return vectordb

def get_vectordb(uploaded_files):
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è·å– API key
    api_key = os.getenv('ZHIPUAI_API_KEY')
    if not api_key:
        st.error("è¯·è®¾ç½®æ™ºè°± AI API key")
        return None
        
    embedding = ZhipuAIEmbeddings(api_key=api_key)
    persist_directory = 'data_base/vector_db/chroma'
    
    if uploaded_files:
        documents = []
        for uploaded_file in uploaded_files:
            try:
                # æ‰“å°æ–‡ä»¶ä¿¡æ¯
                st.write(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {uploaded_file.name}")
                
                # ç›´æ¥ä»ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡è¯»å–ï¼Œä¸ä¿å­˜åˆ°æœ¬åœ°
                pdf_document = PdfReader(uploaded_file)
                text = ""
                for page in pdf_document.pages:
                    text += page.extract_text()
                
                # æ‰“å°æå–çš„æ–‡æœ¬é•¿åº¦
                st.write(f"æå–çš„æ–‡æœ¬é•¿åº¦: {len(text)}")
                
                if text.strip():
                    document = Document(
                        page_content=text,
                        metadata={"source": uploaded_file.name}
                    )
                    documents.append(document)
                    st.sidebar.success(f"{uploaded_file.name} å·²æˆåŠŸå¤„ç†ã€‚")
                else:
                    st.sidebar.warning(f"{uploaded_file.name} æ˜¯ç©ºæ–‡ä»¶ã€‚")
                
            except Exception as e:
                st.sidebar.error(f"å¤„ç† {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # æ–‡æœ¬åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # å¢åŠ å—å¤§å°
            chunk_overlap=200,  # å¢åŠ é‡å éƒ¨åˆ†
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]  # ä¼˜åŒ–åˆ†å‰²ç¬¦
        )
        split_docs = text_splitter.split_documents(documents)
            
        # åˆ›å»ºä¸´æ—¶å‘é‡æ•°æ®åº“ï¼ˆä¸æŒä¹…åŒ–ï¼‰
        vectordb = Chroma.from_documents(
            documents=split_docs,
            embedding=embedding
        )
        return vectordb
    else:
        # å¦‚æœæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ï¼ŒåŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embedding
        )

#å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
def get_chat_qa_chain(input_text, zhipuai_api_key, vectordb):
    llm = ZhipuAILLM(model= "glm-4-flash", temperature = 0,api_key=zhipuai_api_key)
    memory = ConversationBufferMemory(
        memory_key="chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚
        return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
    )
    retriever=vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory
    )
    result = qa.invoke({"question": input_text})
    return result['answer']

#ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
def get_qa_chain(question: str, zhipuai_api_key: str, vectordb, query_mode="å…¨å±€æŸ¥è¯¢", selected_files=None):
    try:
        llm = ZhipuAILLM(model="glm-4", temperature=0.7, api_key=zhipuai_api_key)
        
        # åˆ›å»ºæ£€ç´¢å™¨å¹¶è·å–ç›¸å…³æ–‡æ¡£
        retriever = vectordb.as_retriever(
            search_kwargs={
                "k": 6,  # å¢åŠ æ£€ç´¢æ–‡æ¡£æ•°
                "fetch_k": 10,  # é¢„å–æ›´å¤šæ–‡æ¡£
                "score_threshold": 0.5  # è®¾ç½®ç›¸å…³æ€§é˜ˆå€¼
            }
        )
        docs = retriever.get_relevant_documents(question)
        
        # æ‰“å°æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼ˆè°ƒè¯•ç”¨ï¼‰
        st.write(f"æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        for i, doc in enumerate(docs):
            st.write(f"\næ–‡æ¡£ {i+1}:")
            st.write(f"å†…å®¹: {doc.page_content[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
            st.write(f"æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
        
        # æ·»åŠ æ€è€ƒæ­¥éª¤
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”é—®é¢˜ï¼š

1. ä»”ç»†é˜…è¯»ä¸Šä¸‹æ–‡ä¿¡æ¯
2. åˆ†æå…³é”®ä¿¡æ¯ç‚¹
3. ç»„ç»‡é€»è¾‘æ¡†æ¶
4. æä¾›è¯¦ç»†ç­”æ¡ˆ

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

æ€è€ƒæ­¥éª¤ï¼š
1. ä¸»è¦ä¿¡æ¯ï¼š[åˆ—å‡ºå…³é”®ä¿¡æ¯ç‚¹]
2. ç›¸å…³ç»†èŠ‚ï¼š[è¡¥å……é‡è¦ç»†èŠ‚]
3. é€»è¾‘å…³ç³»ï¼š[åˆ†æä¿¡æ¯ä¹‹é—´çš„è”ç³»]
4. å®Œæ•´å›ç­”ï¼š[åŸºäºä»¥ä¸Šåˆ†æç»™å‡ºå…¨é¢ç­”æ¡ˆ]

å›ç­”ï¼š"""
        
        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question"],
            template=template
        )
        
        # åˆ›å»ºé—®ç­”é“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm,
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
        )
        
        # è·å–ç­”æ¡ˆ
        result = qa_chain.invoke({"query": question})
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        sources = set()
        if hasattr(result, 'source_documents'):
            for doc in result['source_documents']:
                if 'source' in doc.metadata:
                    sources.add(doc.metadata['source'])
        
        answer = result["result"]
        if sources:
            answer += f"\n\nå‚è€ƒæ¥æºï¼š{', '.join(sources)}"
        
        return answer
        
    except Exception as e:
        st.error(f"é—®ç­”å¤„ç†å‡ºé”™: {str(e)}")
        return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ã€‚"


# #åŠ è½½PDFå’ŒMarkdownæ–‡ä»¶
# def load_pdf(file_path):
#     return PyMuPDFLoader(file_path).load()


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    st.title('ggbond_knowledge_base')
    
    # APIå¯†é’¥è¾“å…¥
    zhipuai_api_key = st.sidebar.text_input('Zhipu API Key', type='password')
    
    # æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
    uploaded_files = st.sidebar.file_uploader("ä¸Šä¼ PDFæ–‡ä»¶", type=["pdf"], accept_multiple_files=True)
    
    # ç³»ç»Ÿç»´æŠ¤å·¥å…·æ”¾åœ¨ä¾§è¾¹æ çš„æŠ˜å é¢æ¿ä¸­
    with st.sidebar.expander("ç³»ç»Ÿç»´æŠ¤", expanded=False):
        if st.button("ğŸ“‹ æ£€æŸ¥æ–‡ä»¶çŠ¶æ€", key="check_status"):
            st.write("### æ–‡ä»¶çŠ¶æ€æ£€æŸ¥")
            for category in os.listdir("data_base/knowledge_db"):
                category_path = os.path.join("data_base/knowledge_db", category)
                if os.path.isdir(category_path):
                    st.write(f"\n#### {category} ç±»åˆ«ï¼š")
                    for file in os.listdir(category_path):
                        if file.endswith('.pdf'):
                            file_path = os.path.join(category_path, file)
                            success, message = process_file(file_path, category)
                            if success:
                                st.success(f"âœ… {file} å¤„ç†æ­£å¸¸")
                            else:
                                st.error(f"âŒ {file} å¤„ç†å¤±è´¥: {message}")
        
        if st.button("ğŸ”„ é‡å»ºå‘é‡æ•°æ®åº“", key="rebuild_db"):
            with st.spinner("æ­£åœ¨é‡å»ºå‘é‡æ•°æ®åº“..."):
                if os.path.exists("data_base/vector_db/chroma"):
                    import shutil
                    shutil.rmtree("data_base/vector_db/chroma")
                vectordb = process_knowledge_base()
                st.success("å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼")
    
    # æ·»åŠ æ–‡ä»¶é€‰æ‹©å™¨
    query_mode, selected_files = file_selector()
    
    # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶å¹¶è·å–vectordb
    if uploaded_files:
        vectordb = get_vectordb(uploaded_files)
    else:
        # å°è¯•åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
        try:
            embedding = ZhipuAIEmbeddings()
            vectordb = Chroma(
                persist_directory='data_base/vector_db/chroma',
                embedding_function=embedding
            )
        except Exception as e:
            st.error("åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼Œè¯·é‡æ–°å¤„ç†çŸ¥è¯†åº“ã€‚")
            vectordb = None
    
    # é€‰æ‹©é—®ç­”æ–¹å¼
    selected_method = st.sidebar.selectbox(
        "é€‰æ‹©é—®ç­”æ–¹å¼",
        ["qa_chain", "chat_qa_chain", "chat"]
    )
    
    # åˆå§‹åŒ–èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["text"])
    
    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜"):
        st.session_state.messages.append({"role": "user", "text": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            if selected_method == "qa_chain":
                answer = get_qa_chain(prompt, zhipuai_api_key, vectordb, query_mode, selected_files)
            elif selected_method == "chat_qa_chain":
                answer = get_chat_qa_chain(prompt, zhipuai_api_key, vectordb)
            else:
                answer = generate_response(prompt, zhipuai_api_key)
            
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "text": answer})

def file_selector():
    st.sidebar.markdown("### é€‰æ‹©è¦æŸ¥è¯¢çš„æ–‡ä»¶")
    knowledge_dir = "data_base/knowledge_db"
    
    # æŸ¥è¯¢æ¨¡å¼é€‰æ‹©
    query_mode = st.sidebar.radio(
        "é€‰æ‹©æŸ¥è¯¢æ¨¡å¼",
        ["å…¨å±€æŸ¥è¯¢", "åˆ†ç±»æŸ¥è¯¢"],
        help="å…¨å±€æŸ¥è¯¢å°†æœç´¢æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ†ç±»æŸ¥è¯¢å¯ä»¥é€‰æ‹©ç‰¹å®šç±»åˆ«"
    )
    
    selected_files = []
    if query_mode == "åˆ†ç±»æŸ¥è¯¢":
        # è·å–æ‰€æœ‰åˆ†ç±»ç›®å½•
        categories = [d for d in os.listdir(knowledge_dir) 
                     if os.path.isdir(os.path.join(knowledge_dir, d))]
        
        # é€‰æ‹©åˆ†ç±»
        selected_categories = st.sidebar.multiselect(
            "é€‰æ‹©è¦æŸ¥è¯¢çš„åˆ†ç±»",
            categories,
            help="é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªåˆ†ç±»è¿›è¡ŒæŸ¥è¯¢"
        )
        
        # æ˜¾ç¤ºæ‰€é€‰åˆ†ç±»ä¸‹çš„æ–‡ä»¶
        if selected_categories:
            for category in selected_categories:
                category_path = os.path.join(knowledge_dir, category)
                if os.path.exists(category_path):
                    files = [f for f in os.listdir(category_path) if f.endswith('.pdf')]
                    if files:
                        st.sidebar.markdown(f"#### {category}ç±»æ–‡ä»¶ï¼š")
                        category_files = st.sidebar.multiselect(
                            f"é€‰æ‹©{category}ç±»æ–‡ä»¶",
                            files,
                            key=f"files_{category}"
                        )
                        selected_files.extend([os.path.join(category, f) for f in category_files])
    
    return query_mode, selected_files

def file_manager():
    st.sidebar.markdown("### æ–‡ä»¶ç®¡ç†")
    
    # æ˜¾ç¤ºå·²å­˜å‚¨çš„æ–‡ä»¶
    knowledge_dir = "data_base/knowledge_db"
    if os.path.exists(knowledge_dir):
        files = os.listdir(knowledge_dir)
        if files:
            st.sidebar.markdown("#### å·²å­˜å‚¨çš„æ–‡ä»¶ï¼š")
            for file in files:
                col1, col2 = st.sidebar.columns([3, 1])
                with col1:
                    st.write(file)
                with col2:
                    if st.button("åˆ é™¤", key=f"delete_{file}"):
                        file_path = os.path.join(knowledge_dir, file)
                        try:
                            os.remove(file_path)
                            st.success(f"å·²åˆ é™¤ {file}")
                            # åˆ é™¤åéœ€è¦é‡å»ºå‘é‡æ•°æ®åº“
                            if os.path.exists("data_base/vector_db/chroma"):
                                import shutil
                                shutil.rmtree("data_base/vector_db/chroma")
                            st.rerun()
                        except Exception as e:
                            st.error(f"åˆ é™¤å¤±è´¥ï¼š{str(e)}")

def check_password():
    """è¿”å›`True` å¦‚æœç”¨æˆ·è¾“å…¥æ­£ç¡®çš„å¯†ç """
    def password_entered():
        """æ£€æŸ¥æ˜¯å¦è¾“å…¥äº†æ­£ç¡®çš„å¯†ç """
        if st.session_state["password"] == st.secrets["password"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # ä¸è¦åœ¨session stateä¸­ä¿å­˜å¯†ç 
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        # æ˜¾ç¤ºè¾“å…¥æ¡†
        st.text_input(
            "è¯·è¾“å…¥å¯†ç ", type="password", on_change=password_entered, key="password"
        )
        return False
    elif not st.session_state["password_correct"]:
        # å¯†ç é”™è¯¯
        st.text_input(
            "è¯·è¾“å…¥å¯†ç ", type="password", on_change=password_entered, key="password"
        )
        st.error("ğŸ˜• å¯†ç é”™è¯¯")
        return False
    else:
        # å¯†ç æ­£ç¡®
        return True

def create_category_structure():
    """åˆ›å»ºåˆ†ç±»ç›®å½•ç»“æ„"""
    base_dir = "data_base/knowledge_db"
    categories = {
        "å†å²": "history",
        "ç¤¾ä¼š": "society",
        "æ–‡å­¦": "literature",
        "ç§‘æŠ€": "technology",
        "äººç”Ÿå‘¨æŠ¥":"life_weekly",
        # å¯ä»¥æ·»åŠ æ›´å¤šåˆ†ç±»
    }
    
    # åˆ›å»ºåˆ†ç±»ç›®å½•
    for category in categories.values():
        os.makedirs(os.path.join(base_dir, category), exist_ok=True)
    
    return categories

def enhanced_file_selector():
    """å¢å¼ºç‰ˆæ–‡ä»¶é€‰æ‹©å™¨"""
    st.sidebar.markdown("### çŸ¥è¯†åº“æŸ¥è¯¢è®¾ç½®")
    
    # è·å–æ‰€æœ‰åˆ†ç±»
    categories = create_category_structure()
    base_dir = "data_base/knowledge_db"
    
    # æŸ¥è¯¢æ¨¡å¼é€‰æ‹©
    query_mode = st.sidebar.radio(
        "é€‰æ‹©æŸ¥è¯¢æ¨¡å¼",
        ["å…¨å±€æŸ¥è¯¢", "åˆ†ç±»æŸ¥è¯¢"],
        help="å…¨å±€æŸ¥è¯¢å°†æœç´¢æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ†ç±»æŸ¥è¯¢å¯ä»¥é€‰æ‹©ç‰¹å®šç±»åˆ«"
    )
    
    selected_files = []
    if query_mode == "åˆ†ç±»æŸ¥è¯¢":
        # é€‰æ‹©åˆ†ç±»
        selected_categories = st.sidebar.multiselect(
            "é€‰æ‹©è¦æŸ¥è¯¢çš„åˆ†ç±»",
            list(categories.keys()),
            help="é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªåˆ†ç±»è¿›è¡ŒæŸ¥è¯¢"
        )
        
        # æ˜¾ç¤ºæ‰€é€‰åˆ†ç±»ä¸‹çš„æ–‡ä»¶
        if selected_categories:
            for category in selected_categories:
                category_path = os.path.join(base_dir, categories[category])
                if os.path.exists(category_path):
                    files = os.listdir(category_path)
                    if files:
                        st.sidebar.markdown(f"#### {category}ç±»æ–‡ä»¶ï¼š")
                        category_files = st.sidebar.multiselect(
                            f"é€‰æ‹©{category}ç±»æ–‡ä»¶",
                            files,
                            key=f"files_{category}"
                        )
                        selected_files.extend([os.path.join(categories[category], f) for f in category_files])
    
    return query_mode, selected_files

def process_file(file_path, category):
    """å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶è¿”å›å¤„ç†çŠ¶æ€"""
    try:
        # è¯»å–PDF
        reader = PdfReader(file_path)
        text = ""
        total_pages = len(reader.pages)
        
        if total_pages == 0:
            return False, "PDFæ–‡ä»¶ä¸ºç©º"
            
        # è¯»å–æ‰€æœ‰é¡µé¢
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
                
        # æ£€æŸ¥æå–çš„æ–‡æœ¬
        if not text.strip():
            return False, "æ— æ³•æå–æ–‡æœ¬å†…å®¹"
            
        # è¿”å›æˆåŠŸçŠ¶æ€å’Œæ–‡æœ¬é•¿åº¦ä¿¡æ¯
        return True, f"æˆåŠŸæå– {len(text)} å­—ç¬¦çš„æ–‡æœ¬"
        
    except Exception as e:
        return False, f"å¤„ç†å‡ºé”™: {str(e)}"

def process_knowledge_base():
    """å¤„ç†çŸ¥è¯†åº“ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""
    knowledge_dir = "data_base/knowledge_db"
    documents = []
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(knowledge_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰å­ç›®å½•ï¼ˆåˆ†ç±»ç›®å½•ï¼‰
    categories = [d for d in os.listdir(knowledge_dir) 
                 if os.path.isdir(os.path.join(knowledge_dir, d))]
    
    if not categories:
        st.warning("çŸ¥è¯†åº“ç›®å½•ä¸­æ²¡æœ‰åˆ†ç±»ç›®å½•ã€‚")
        return None
    
    # æ˜¾ç¤ºå¤„ç†è¿›åº¦
    progress_text = st.empty()
    progress_bar = st.progress(0)
    total_files = 0
    processed_files = 0
    
    # è®¡ç®—æ€»æ–‡ä»¶æ•°
    for category in categories:
        category_path = os.path.join(knowledge_dir, category)
        pdf_files = [f for f in os.listdir(category_path) if f.endswith('.pdf')]
        total_files += len(pdf_files)
    
    # å¤„ç†æ¯ä¸ªåˆ†ç±»ä¸‹çš„æ–‡ä»¶
    for category in categories:
        category_path = os.path.join(knowledge_dir, category)
        pdf_files = [f for f in os.listdir(category_path) if f.endswith('.pdf')]
        
        for pdf_file in pdf_files:
            try:
                file_path = os.path.join(category_path, pdf_file)
                progress_text.text(f"æ­£åœ¨å¤„ç†: {category}/{pdf_file}")
                
                # ä½¿ç”¨ PyMuPDF åˆ†æ‰¹è¯»å–
                reader = PdfReader(file_path)
                total_pages = len(reader.pages)
                
                # åˆ†æ‰¹å¤„ç†é¡µé¢
                batch_size = 10  # æ¯æ‰¹å¤„ç†10é¡µ
                for page_start in range(0, total_pages, batch_size):
                    batch_text = ""
                    page_end = min(page_start + batch_size, total_pages)
                    
                    for page_num in range(page_start, page_end):
                        try:
                            page = reader.pages[page_num]
                            batch_text += page.extract_text() + "\n"
                        except Exception as e:
                            st.warning(f"{pdf_file} ç¬¬ {page_num + 1} é¡µå¤„ç†å‡ºé”™: {str(e)}")
                            continue
                    
                    if batch_text.strip():
                        documents.append(Document(
                            page_content=batch_text,
                            metadata={
                                "source": f"{category}/{pdf_file}",
                                "category": category,
                                "page_range": f"{page_start+1}-{page_end}"
                            }
                        ))
                
                processed_files += 1
                progress = processed_files / total_files
                progress_bar.progress(progress)
                
            except Exception as e:
                st.error(f"å¤„ç† {pdf_file} æ—¶å‡ºé”™: {str(e)}")
                continue
    
    if not documents:
        st.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£ã€‚")
        return None
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“
    try:
        # ç¡®ä¿ API key å­˜åœ¨
        api_key = os.getenv('ZHIPUAI_API_KEY')
        if not api_key:
            st.error("æœªæ‰¾åˆ° ZHIPUAI_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
            return None
            
        # åˆ›å»º embedding å®ä¾‹
        embedding = ZhipuAIEmbeddings(api_key=api_key)
        
        # åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        
        split_docs = text_splitter.split_documents(documents)
        
        if split_docs:
            persist_directory = 'data_base/vector_db/chroma'
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(persist_directory, exist_ok=True)
            
            # åˆ›å»ºå‘é‡æ•°æ®åº“
            vectordb = Chroma.from_documents(
                documents=split_docs,
                embedding=embedding,
                persist_directory=persist_directory
            )
            
            # æŒä¹…åŒ–ä¿å­˜
            vectordb.persist()
            
            # æ˜¾ç¤ºæˆåŠŸä¿¡æ¯
            st.success(f"æˆåŠŸå¤„ç† {len(split_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µï¼")
            return vectordb
        else:
            st.error("æ–‡æ¡£åˆ†å‰²åä¸ºç©ºã€‚")
            return None
            
    except Exception as e:
        st.error(f"åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        return None

if __name__ == "__main__":
    main()
